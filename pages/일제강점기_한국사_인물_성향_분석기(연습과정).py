# -*- coding: utf-8 -*-
"""ì¼ì œê°•ì ê¸° í•œêµ­ì‚¬ ì¸ë¬¼ ì„±í–¥ ë¶„ì„ê¸°(ì—°ìŠµê³¼ì •).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10WDYnOVi9LTDGQ0jJf-1WptSLXOGZ9rl
"""

pip install requests beautifulsoup4 pandas scikit-learn konlpy

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# ==========================================
# 1. ë°ì´í„° ìˆ˜ì§‘ (Web Scraping Stub)
# ==========================================
def get_historical_data(keyword):
    """
    í•œêµ­ì‚¬ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í‚¤ì›Œë“œ(ì¸ë¬¼ëª…)ë¡œ ê²€ìƒ‰í•˜ì—¬ ê´€ë ¨ ì‚¬ë£Œ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    (ì‹¤ì œ ë™ì‘ì„ ìœ„í•´ì„œëŠ” í•´ë‹¹ ì‚¬ì´íŠ¸ì˜ êµ¬ì²´ì ì¸ URL íŒ¨í„´ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.
     ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ URL êµ¬ì¡°ë¥¼ ê°€ì •í•˜ì—¬ ì‘ì„±í•©ë‹ˆë‹¤.)
    """
    base_url = "https://db.history.go.kr/search/search.do"
    params = {'keyword': keyword}

    try:
        # ì‹¤ì œ ìš”ì²­ ì‹œì—ëŠ” User-Agent í—¤ë”ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(base_url, params=params, headers=headers)

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # ì£¼ì˜: ì•„ë˜ ì„ íƒì(.text_body)ëŠ” ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ì¶° ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.
            # ì˜ˆì‹œ: ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì‚¬ë£Œ ë‚´ìš©ì„ ë‹´ê³  ìˆëŠ” íƒœê·¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            contents = soup.select(".text_body")

            full_text = " ".join([c.get_text() for c in contents])
            return full_text.strip()
        else:
            return ""
    except Exception as e:
        print(f"Error scraping {keyword}: {e}")
        return ""

# ==========================================
# 2. ê°€ìƒ ë°ì´í„° ìƒì„± (í•™ìŠµì„ ìœ„í•œ ì˜ˆì‹œ)
# ==========================================
# ì‹¤ì œë¡œëŠ” ìœ„ í¬ë¡¤ë§ í•¨ìˆ˜ë¡œ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ,
# ì½”ë“œê°€ ë°”ë¡œ ì‹¤í–‰ë˜ë„ë¡ í•˜ê¸° ìœ„í•´ 'ê°€ìƒì˜ ì—­ì‚¬ ì‚¬ë£Œ í…ìŠ¤íŠ¸'ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

data = [
    # ë¬´ì¥íˆ¬ìŸë¡  (Armed Struggle)
    {"text": "ë§Œì£¼ì—ì„œ ë…ë¦½êµ°ì„ ì¡°ì§í•˜ì—¬ ì¼ë³¸êµ°ê³¼ ì „íˆ¬ë¥¼ ë²Œì˜€ë‹¤. ë¬´ë ¥ìœ¼ë¡œ ë…ë¦½ì„ ìŸì·¨í•´ì•¼ í•œë‹¤. ì´ê³¼ í­íƒ„ìœ¼ë¡œ ì €í•­í–ˆë‹¤.", "label": "ë¬´ì¥íˆ¬ìŸë¡ "},
    {"text": "ì˜ì—´ë‹¨ì„ ì¡°ì§í•˜ì—¬ ì‹ë¯¼ í†µì¹˜ ê¸°ê´€ì„ íŒŒê´´í•˜ê³  ìš”ì¸ì„ ì•”ì‚´í•˜ëŠ” ë¬´ì¥ íˆ¬ìŸì„ ì „ê°œí–ˆë‹¤.", "label": "ë¬´ì¥íˆ¬ìŸë¡ "},
    {"text": "ë´‰ì˜¤ë™ ì „íˆ¬ì™€ ì²­ì‚°ë¦¬ ëŒ€ì²©ì—ì„œ ì¼ë³¸ ì •ê·œêµ°ì„ ìƒëŒ€ë¡œ ëŒ€ìŠ¹ì„ ê±°ë‘ì—ˆë‹¤. êµ°ì‚¬ í›ˆë ¨ê³¼ ë¬´ê¸° êµ¬ì…ì— í˜ì¼ë‹¤.", "label": "ë¬´ì¥íˆ¬ìŸë¡ "},

    # ì‹¤ë ¥ì–‘ì„±ë¡  (Ability Cultivation)
    {"text": "êµìœ¡ê³¼ ì‚°ì—…ì„ ì§„í¥ì‹œì¼œ ë¯¼ì¡±ì˜ ì‹¤ë ¥ì„ ê¸¸ëŸ¬ì•¼ í•œë‹¤. í•™êµë¥¼ ì„¤ë¦½í•˜ê³  ë¯¼ì¡± ìë³¸ì„ ìœ¡ì„±í•˜ì.", "label": "ì‹¤ë ¥ì–‘ì„±ë¡ "},
    {"text": "ë¬¼ì‚°ì¥ë ¤ìš´ë™ì„ í†µí•´ êµ­ì‚°í’ˆì„ ì• ìš©í•˜ê³  ê²½ì œì  ìë¦½ì„ ê¾€í–ˆë‹¤. ë¬¸ë§¹ í‡´ì¹˜ì™€ ê³„ëª½ ìš´ë™ì´ ê¸‰ì„ ë¬´ë‹¤.", "label": "ì‹¤ë ¥ì–‘ì„±ë¡ "},
    {"text": "ëŒ€ì„±í•™êµë¥¼ ì„¸ì›Œ ì²­ë…„ë“¤ì„ êµìœ¡í•˜ê³ , í¥ì‚¬ë‹¨ì„ í†µí•´ ì¸ì¬ë¥¼ ì–‘ì„±í•˜ì—¬ í›—ë‚ ì˜ ë…ë¦½ì„ ì¤€ë¹„í–ˆë‹¤.", "label": "ì‹¤ë ¥ì–‘ì„±ë¡ "},

    # ì™¸êµë¡  (Diplomacy)
    {"text": "ë¯¸êµ­ê³¼ ì—´ê°•ì— í•œêµ­ì˜ ë…ë¦½ ì˜ì§€ë¥¼ í˜¸ì†Œí•˜ê³  ì²­ì›ì„œë¥¼ ì œì¶œí–ˆë‹¤. êµ­ì œ íšŒì˜ì— íŒŒê²¬ë˜ì–´ ì™¸êµì  ë…¸ë ¥ì„ ê¸°ìš¸ì˜€ë‹¤.", "label": "ì™¸êµë¡ "},
    {"text": "êµ­ì œ ì—°ë§¹ì— ë…ë¦½ì„ ì²­ì›í•˜ê³ , êµ¬ë¯¸ ìœ„ì›ë¶€ë¥¼ ì„¤ì¹˜í•˜ì—¬ ì„ ì „ í™œë™ì„ í–ˆë‹¤. ì™¸êµì ì¸ ì§€ì§€ë¥¼ ì–»ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.", "label": "ì™¸êµë¡ "},
    {"text": "ì´ìŠ¹ë§Œì€ ë¯¸êµ­ ì›Œì‹±í„´ì—ì„œ ì™¸êµ í™œë™ì„ ì£¼ë„í•˜ë©° ìœ„ì„ í†µì¹˜ ì²­ì›ì„ ì‹œë„í•˜ê¸°ë„ í–ˆë‹¤.", "label": "ì™¸êµë¡ "},

    # ì¹œì¼/ë°˜ë¯¼ì¡±í–‰ìœ„ (Pro-Japanese)
    {"text": "ì¡°ì„ ì´ë…ë¶€ì˜ ì •ì±…ì„ ì°¬ì–‘í•˜ê³  ë‚´ì„ ì¼ì²´ë¥¼ ì£¼ì¥í–ˆë‹¤. í•™ë„ë³‘ ì§€ì›ì„ ê¶Œìœ í•˜ëŠ” ì—°ì„¤ì„ í–ˆë‹¤.", "label": "ì¹œì¼íŒŒ"},
    {"text": "ì¤‘ì¶”ì› ì°¸ì˜ë¥¼ ì§€ë‚´ë©° ì¼ì œì˜ ì‹ë¯¼ í†µì¹˜ì— í˜‘ë ¥í–ˆë‹¤. ì²œí™©ì—ê²Œ ì¶©ì„±ì„ ë§¹ì„¸í•˜ê³  í™©êµ­ ì‹ ë¯¼ì´ ë  ê²ƒì„ ê°•ìš”í–ˆë‹¤.", "label": "ì¹œì¼íŒŒ"},
    {"text": "ì¼ì œ ê°•ì ê¸° ë§ê¸° ì¹œì¼ ë‹¨ì²´ì— ê°€ì…í•˜ì—¬ ì „ìŸ ë¬¼ì ê³µì¶œê³¼ ì§•ë³‘ì„ ë…ë ¤í•˜ëŠ” ê¸€ì„ ê¸°ê³ í–ˆë‹¤.", "label": "ì¹œì¼íŒŒ"}
]

df = pd.DataFrame(data)

# ==========================================
# 3. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ëª¨ë¸ë§
# ==========================================

# í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” (Okt ì‚¬ìš©)
okt = Okt()

def preprocess_text(text):
    """
    í•œê¸€ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ì—¬ ì •ì œí•˜ëŠ” í•¨ìˆ˜
    """
    # 1. í•œê¸€ë§Œ ë‚¨ê¸°ê¸°
    text = re.sub(r"[^ê°€-í£\s]", "", text)
    # 2. ëª…ì‚¬ ì¶”ì¶œ
    nouns = okt.nouns(text)
    # 3. ë¶ˆìš©ì–´ ì²˜ë¦¬ (ì˜ˆì‹œ) - í•„ìš”ì‹œ ì¶”ê°€
    stopwords = ['ê²ƒ', 'ìˆ˜', 'ë“±', 'ìœ„í•´', 'í†µí•´']
    nouns = [n for n in nouns if n not in stopwords and len(n) > 1]

    return " ".join(nouns)

# ë°ì´í„° ì „ì²˜ë¦¬ ì ìš©
df['cleaned_text'] = df['text'].apply(preprocess_text)

print(">> ì „ì²˜ë¦¬ëœ ë°ì´í„° ì˜ˆì‹œ:")
print(df[['label', 'cleaned_text']].head())
print("-" * 50)

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(
    df['cleaned_text'], df['label'], test_size=0.2, random_state=42
)

# íŒŒì´í”„ë¼ì¸ êµ¬ì¶•: TF-IDF ë²¡í„°í™” -> Naive Bayes ë¶„ë¥˜ê¸°
# Naive BayesëŠ” í…ìŠ¤íŠ¸ ë¶„ë¥˜ì— ìˆì–´ ì ì€ ë°ì´í„°ë¡œë„ ê½¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
model = make_pipeline(
    TfidfVectorizer(),
    MultinomialNB()
)

# ëª¨ë¸ í•™ìŠµ
model.fit(X_train, y_train)

# ==========================================
# 4. ëª¨ë¸ í‰ê°€ ë° ì˜ˆì¸¡
# ==========================================

# í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
predicted = model.predict(X_test)
print(">> ëª¨ë¸ í‰ê°€ ë¦¬í¬íŠ¸:")
print(classification_report(y_test, predicted))

def predict_person_tendency(text):
    """
    ìƒˆë¡œìš´ í…ìŠ¤íŠ¸(ì‚¬ë£Œ)ê°€ ë“¤ì–´ì™”ì„ ë•Œ ì„±í–¥ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜
    """
    processed = preprocess_text(text)
    category = model.predict([processed])[0]
    proba = model.predict_proba([processed]).max()

    return category, proba

# --- ì‹¤ì œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ---
test_sentences = [
    "ìƒí•´ ì„ì‹œì •ë¶€ì—ì„œ í™œë™í•˜ë©° í­íƒ„ ì˜ê±°ë¥¼ ê³„íší•˜ê³  ì‹¤í–‰ì— ì˜®ê²¼ë‹¤.",  # ì˜ˆìƒ: ë¬´ì¥íˆ¬ìŸë¡ 
    "ë¯¼ë¦½ëŒ€í•™ ì„¤ë¦½ ìš´ë™ì„ ì£¼ë„í•˜ê³  ì‹ ë¬¸ì‚¬ë¥¼ í†µí•´ ë¯¼ì¡± ê³„ëª½ì— ì•ì¥ì„°ë‹¤.", # ì˜ˆìƒ: ì‹¤ë ¥ì–‘ì„±ë¡ 
    "ì¼ë³¸ ì œêµ­ ì˜íšŒì— ì°¸ì„í•˜ì—¬ ë‚´ì„ ìœµí™”ë¥¼ ê°•ì¡°í•˜ê³  ì „ìŸ í˜‘ë ¥ì„ ì£¼ì¥í–ˆë‹¤." # ì˜ˆìƒ: ì¹œì¼íŒŒ
]

print("-" * 50)
print(">> ìƒˆë¡œìš´ ì‚¬ë£Œ ì˜ˆì¸¡ ê²°ê³¼:")
for sent in test_sentences:
    cat, conf = predict_person_tendency(sent)
    print(f"ë¬¸ì¥: {sent[:30]}...")
    print(f"ì˜ˆì¸¡ ì„±í–¥: {cat} (í™•ì‹ ë„: {conf:.2f})")
    print("")

import pandas as pd
import re
import warnings
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")

# ==========================================
# 1. ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ ì¤€ë¹„ (Training Data)
# ==========================================
# ëª¨ë¸ì´ ê° ì„±í–¥ì˜ íŠ¹ì§•(í‚¤ì›Œë“œ)ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ê¸°ì´ˆ ë°ì´í„°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
train_data = [
    # [ë¬´ì¥íˆ¬ìŸë¡ ]
    {"text": "ë§Œì£¼ì™€ ì—°í•´ì£¼ì—ì„œ ë…ë¦½êµ°ì„ ì¡°ì§í•˜ì—¬ ì¼ë³¸êµ°ê³¼ ë¬´ë ¥ìœ¼ë¡œ ì „íˆ¬ë¥¼ ë²Œì˜€ë‹¤. ì˜ì—´ë‹¨ì„ ì¡°ì§í•˜ì—¬ ìš”ì¸ ì•”ì‚´ê³¼ ì‹ë¯¼ í†µì¹˜ ê¸°ê´€ íŒŒê´´ í™œë™ì„ ì „ê°œí–ˆë‹¤.", "label": "ë¬´ì¥íˆ¬ìŸë¡ "},
    {"text": "ì²­ì‚°ë¦¬ ëŒ€ì²©ê³¼ ë´‰ì˜¤ë™ ì „íˆ¬ì—ì„œ ì¼ë³¸êµ°ì„ ê²©íŒŒí–ˆë‹¤. ëŒ€í•œë…ë¦½êµ°ê³¼ ê´‘ë³µêµ°ì„ ì°½ì„¤í•˜ì—¬ í•­ì¼ ë¬´ì¥ íˆ¬ìŸì„ ì´ëŒì—ˆë‹¤.", "label": "ë¬´ì¥íˆ¬ìŸë¡ "},
    {"text": "í­íƒ„ íˆ¬ì²™ê³¼ ì´ê²©ì „ì„ í†µí•´ ì¼ì œì— ì €í•­í–ˆë‹¤. êµ°ì‚¬ í›ˆë ¨ì„ í†µí•´ ì •ê·œêµ°ì„ ì–‘ì„±í•˜ê³  ì „ìŸì„ ì¤€ë¹„í•´ì•¼ í•œë‹¤.", "label": "ë¬´ì¥íˆ¬ìŸë¡ "},

    # [ì‹¤ë ¥ì–‘ì„±ë¡ ]
    {"text": "êµìœ¡ì„ í†µí•´ ë¯¼ì¡±ì˜ ì§€ì‹ì„ ê¹¨ìš°ì¹˜ê³  ì‚°ì—…ì„ ì¼ìœ¼ì¼œ ê²½ì œì  ìë¦½ì„ ì´ë¤„ì•¼ í•œë‹¤. ë¬¼ì‚°ì¥ë ¤ìš´ë™ì„ ì£¼ë„í–ˆë‹¤.", "label": "ì‹¤ë ¥ì–‘ì„±ë¡ "},
    {"text": "í•™êµë¥¼ ì„¤ë¦½í•˜ì—¬ ì¸ì¬ë¥¼ ì–‘ì„±í•˜ê³  ë¯¼ì¡± ìë³¸ì„ ìœ¡ì„±í•˜ëŠ” ê²ƒì´ ê¸‰ì„ ë¬´ë‹¤. ë¬¸ë§¹ í‡´ì¹˜ ìš´ë™ê³¼ ë¸Œë‚˜ë¡œë“œ ìš´ë™ì„ ì „ê°œí–ˆë‹¤.", "label": "ì‹¤ë ¥ì–‘ì„±ë¡ "},
    {"text": "ë‹¹ì¥ì˜ ë…ë¦½ë³´ë‹¤ëŠ” ë¯¼ì¡±ì˜ ì‹¤ë ¥ì„ ê¸°ë¥´ëŠ” ê²ƒì´ ìš°ì„ ì´ë‹¤. í¥ì‚¬ë‹¨ê³¼ ê°™ì€ ë‹¨ì²´ë¥¼ í†µí•´ ì²­ë…„ë“¤ì„ êµìœ¡í–ˆë‹¤.", "label": "ì‹¤ë ¥ì–‘ì„±ë¡ "},

    # [ì™¸êµë¡ ]
    {"text": "ë¯¸êµ­ê³¼ ìœ ëŸ½ ì—´ê°•ì— í•œêµ­ì˜ ë…ë¦½ í•„ìš”ì„±ì„ í˜¸ì†Œí–ˆë‹¤. íŒŒë¦¬ ê°•í™” íšŒì˜ì— ëŒ€í‘œë¥¼ íŒŒê²¬í•˜ì—¬ ì²­ì›ì„œë¥¼ ì œì¶œí–ˆë‹¤.", "label": "ì™¸êµë¡ "},
    {"text": "êµ­ì œ ì—°ë§¹ê³¼ ë¯¸êµ­ ëŒ€í†µë ¹ì—ê²Œ ìœ„ì„ í†µì¹˜ë¥¼ ì²­ì›í•˜ê±°ë‚˜ ì™¸êµì  ì§€ì›ì„ ìš”ì²­í–ˆë‹¤. êµ¬ë¯¸ ìœ„ì›ë¶€ë¥¼ ì„¤ì¹˜í•˜ì—¬ ì„ ì „ í™œë™ì„ í–ˆë‹¤.", "label": "ì™¸êµë¡ "},
    {"text": "êµ­ì œ ì‚¬íšŒì˜ ì—¬ë¡ ì„ í™˜ê¸°ì‹œí‚¤ê³  ê°•ëŒ€êµ­ì˜ í˜ì„ ë¹Œë ¤ ë…ë¦½ì„ ìŸì·¨í•˜ê³ ì í–ˆë‹¤. ì›Œì‹±í„´ì—ì„œ ì™¸êµ í™œë™ì„ ì£¼ë ¥í–ˆë‹¤.", "label": "ì™¸êµë¡ "},

    # [ì¹œì¼íŒŒ/ë°˜ë¯¼ì¡±í–‰ìœ„]
    {"text": "ì¡°ì„ ì´ë…ë¶€ì˜ ì‹ë¯¼ í†µì¹˜ ì •ì±…ì„ ì§€ì§€í•˜ê³  ë‚´ì„ ì¼ì²´ë¥¼ ê°•ì¡°í–ˆë‹¤. ì¼ë³¸ ì²œí™©ì—ê²Œ ì¶©ì„±ì„ ë§¹ì„¸í•˜ê³  ì‹ ì‚¬ ì°¸ë°°ë¥¼ ê°•ìš”í–ˆë‹¤.", "label": "ì¹œì¼íŒŒ"},
    {"text": "í•™ë„ë³‘ ì§€ì›ì„ ê¶Œìœ í•˜ëŠ” ê°•ì—°ì„ í•˜ê³  êµ­ë°© í—Œê¸ˆì„ í—Œë‚©í–ˆë‹¤. íƒœí‰ì–‘ ì „ìŸì„ ì°¬ì–‘í•˜ë©° ì¼ì œì˜ ì¹¨ëµ ì „ìŸì— í˜‘ë ¥í–ˆë‹¤.", "label": "ì¹œì¼íŒŒ"},
    {"text": "ì¤‘ì¶”ì› ì°¸ì˜ë¥¼ ì§€ë‚´ë©° í›ˆì¥ì„ ë°›ì•˜ê³ , ì¹œì¼ ë‹¨ì²´ë¥¼ ì¡°ì§í•˜ì—¬ í™©êµ­ ì‹ ë¯¼í™” ì •ì±…ì— ì•ì¥ì„°ë‹¤.", "label": "ì¹œì¼íŒŒ"}
]

# í•™ìŠµ ë°ì´í„° í”„ë ˆì„ ìƒì„±
df_train = pd.DataFrame(train_data)

# ==========================================
# 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµ
# ==========================================
print(">> ëª¨ë¸ì„ í•™ìŠµ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")

okt = Okt()

def preprocess_text(text):
    """í•œê¸€ ëª…ì‚¬ ì¶”ì¶œ ë° ì •ì œ"""
    # í•œê¸€ ì´ì™¸ ì œê±°
    text = re.sub(r"[^ê°€-í£\s]", "", text)
    # ëª…ì‚¬ ì¶”ì¶œ
    nouns = okt.nouns(text)
    # ë¶ˆìš©ì–´ ì²˜ë¦¬ (í•œ ê¸€ì ì œì™¸ ë“±)
    nouns = [n for n in nouns if len(n) > 1]
    return " ".join(nouns)

# í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬
df_train['cleaned_text'] = df_train['text'].apply(preprocess_text)

# ëª¨ë¸ íŒŒì´í”„ë¼ì¸ (TF-IDF + Naive Bayes)
model = make_pipeline(
    TfidfVectorizer(),
    MultinomialNB(alpha=0.1) # alphaëŠ” ìŠ¤ë¬´ë”© íŒŒë¼ë¯¸í„°
)

# ëª¨ë¸ í•™ìŠµ
model.fit(df_train['cleaned_text'], df_train['label'])
print(">> ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n")

# ==========================================
# 3. [í•µì‹¬] ì¸ë¬¼ ë°ì´í„° ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ í•¨ìˆ˜
# ==========================================
def search_person_info(name):
    """
    ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì´ë¦„ì— í•´ë‹¹í•˜ëŠ” ì—­ì‚¬ì  í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜.
    (ì‹¤ì œë¡œëŠ” ì›¹ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•´ì•¼ í•˜ì§€ë§Œ, ë°ëª¨ë¥¼ ìœ„í•´ ì£¼ìš” ì¸ë¬¼ì— ëŒ€í•œ ê°€ìƒ DBë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤)
    """

    # ì‹¤ì œ í¬ë¡¤ë§ ì‹œì—ëŠ” ì•„ë˜ URLì„ í™œìš©í•˜ì—¬ requests + BeautifulSoup ì½”ë“œë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    # url = f"https://db.history.go.kr/search/search.do?keyword={name}"

    # [ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë² ì´ìŠ¤]
    # ì‹¤ì œë¡œëŠ” ì´ í…ìŠ¤íŠ¸ë“¤ì´ ì›¹ì—ì„œ ìŠ¤í¬ë˜í•‘ëœ ê²°ê³¼ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.
    mock_db = {
        "ê¹€êµ¬": """
            ëŒ€í•œë¯¼êµ­ ì„ì‹œì •ë¶€ì˜ ì£¼ì„ì„ ì—­ì„í–ˆë‹¤. í•œì¸ì• êµ­ë‹¨ì„ ì¡°ì§í•˜ì—¬ ì´ë´‰ì°½, ìœ¤ë´‰ê¸¸ ì˜ì‚¬ì˜ ì˜ê±°ë¥¼ ì§€íœ˜í–ˆë‹¤.
            ë°±ë²”ì¼ì§€ë¥¼ ì €ìˆ í•˜ì˜€ìœ¼ë©°, ëê¹Œì§€ ì„ì‹œì •ë¶€ë¥¼ ì§€í‚¤ë©° í•­ì¼ ë¬´ì¥ íˆ¬ìŸê³¼ ë…ë¦½ ìš´ë™ì„ ì´ëŒì—ˆë‹¤.
            êµ°ì‚¬ ì¡°ì§ì¸ í•œêµ­ê´‘ë³µêµ°ì„ ì°½ì„¤í•˜ì—¬ ëŒ€ì¼ ì„ ì „ í¬ê³ ë¥¼ í•˜ê³  ì§„ê³µ ì‘ì „ì„ ì¤€ë¹„í–ˆë‹¤.
        """,
        "ì•ˆì°½í˜¸": """
            ë¯¼ì¡±ì˜ ì‹¤ë ¥ì„ ê¸°ë¥´ê¸° ìœ„í•´ ëŒ€ì„±í•™êµë¥¼ ì„¤ë¦½í•˜ê³  ì²­ë…„ í•™ìš°íšŒë¥¼ ì¡°ì§í–ˆë‹¤.
            ë¯¸êµ­ì—ì„œ í¥ì‚¬ë‹¨ì„ ì°½ì„¤í•˜ì—¬ ë¯¼ì¡± ì§€ë„ìë¥¼ ì–‘ì„±í•˜ëŠ” ë° í˜ì¼ë‹¤.
            ë…ë¦½ì„ ìœ„í•´ì„œëŠ” ê±°ì§“ë§ì„ í•˜ì§€ ë§ê³  ì •ì§í•´ì•¼ í•˜ë©°, í˜ì„ ê¸¸ëŸ¬ì•¼ í•œë‹¤ê³  ì£¼ì¥í•˜ëŠ” ì‹¤ë ¥ ì–‘ì„± ìš´ë™ì˜ ì§€ë„ìì˜€ë‹¤.
        """,
        "ì´ìŠ¹ë§Œ": """
            ë¯¸êµ­ì—ì„œ ì£¼ë¡œ í™œë™í•˜ë©° ì™¸êµì ì¸ ë°©ë²•ìœ¼ë¡œ ë…ë¦½ì„ ë‹¬ì„±í•˜ë ¤ í–ˆë‹¤.
            êµ­ì œ ì—°ë§¹ì— ìœ„ì„ í†µì¹˜ ì²­ì›ì„œë¥¼ ì œì¶œí•˜ì˜€ìœ¼ë©°, êµ¬ë¯¸ ìœ„ì›ë¶€ì—ì„œ í™œë™í•˜ë©° ë¯¸êµ­ì˜ ì§€ì›ì„ ìš”ì²­í–ˆë‹¤.
            ëŒ€í•œë¯¼êµ­ ì„ì‹œì •ë¶€ì˜ ì´ˆëŒ€ ëŒ€í†µë ¹ìœ¼ë¡œ ì„ ì¶œë˜ì—ˆìœ¼ë‚˜ íƒ„í•µë˜ê¸°ë„ í–ˆë‹¤.
        """,
        "ê¹€ì›ë´‰": """
            ì˜ì—´ë‹¨ì„ ì¡°ì§í•˜ì—¬ ì¼ì œì˜ ìˆ˜íƒˆ ê¸°ê´€ íŒŒê´´ì™€ ìš”ì¸ ì•”ì‚´ ë“± ì§ì ‘ì ì¸ ë¬´ì¥ íˆ¬ìŸì„ ì „ê°œí–ˆë‹¤.
            ì¡°ì„ ì˜ìš©ëŒ€ë¥¼ ì°½ì„¤í•˜ì—¬ ì¤‘êµ­ ê´€ë‚´ì—ì„œ ë¬´ì¥ ë…ë¦½ ì „ìŸì„ ìˆ˜í–‰í–ˆë‹¤.
            ê°•ë ¥í•œ í­ë ¥ íˆ¬ìŸë§Œì´ ë…ë¦½ì„ ìŸì·¨í•  ìˆ˜ ìˆë‹¤ê³  ë¯¿ì—ˆë‹¤.
        """,
        "ì´ì™„ìš©": """
            ì„ì‚¬ëŠ‘ì•½ ì²´ê²°ì„ ì£¼ë„í•˜ì—¬ ì™¸êµê¶Œì„ ì¼ë³¸ì— ë„˜ê²¼ë‹¤.
            í•œì¼ í•©ë³‘ ì¡°ì•½ì— ì„œëª…í•˜ì—¬ ë‚˜ë¼ë¥¼ íŒ”ì•„ë„˜ê¸´ ëŒ€í‘œì ì¸ ë§¤êµ­ë…¸ì´ë‹¤.
            ì¼ì œë¡œë¶€í„° ì‘ìœ„ë¥¼ ë°›ê³  ì¤‘ì¶”ì› ê³ ë¬¸ìœ¼ë¡œ í™œë™í•˜ë©° ì¼ë³¸ì˜ ì‹ë¯¼ í†µì¹˜ë¥¼ ì ê·¹ ì§€ì§€í–ˆë‹¤.
        """
    }

    # DBì— ìˆìœ¼ë©´ í•´ë‹¹ í…ìŠ¤íŠ¸ ë°˜í™˜, ì—†ìœ¼ë©´ None ë°˜í™˜
    return mock_db.get(name, None)

# ==========================================
# 4. ì‚¬ìš©ì ì…ë ¥ ë° ì˜ˆì¸¡ ì‹¤í–‰
# ==========================================
def predict_tendency():
    print("=" * 60)
    print("   [ ì¼ì œê°•ì ê¸° ì¸ë¬¼ ì„±í–¥ ë¶„ë¥˜ê¸° ]")
    print("   * ì…ë ¥ ê°€ëŠ¥ ì˜ˆì‹œ: ê¹€êµ¬, ì•ˆì°½í˜¸, ì´ìŠ¹ë§Œ, ê¹€ì›ë´‰, ì´ì™„ìš©")
    print("   * ì¢…ë£Œí•˜ë ¤ë©´ 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("=" * 60)

    while True:
        name = input("\n>> ì¸ë¬¼ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()

        if name == "ì¢…ë£Œ":
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if not name:
            continue

        # 1. ì •ë³´ ê²€ìƒ‰ (í¬ë¡¤ë§ ì‹œë®¬ë ˆì´ì…˜)
        text_data = search_person_info(name)

        if text_data is None:
            print(f"âš  '{name}'ì— ëŒ€í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì˜ˆì‹œ ì¸ë¬¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”)")
            continue

        print(f"\n[ìˆ˜ì§‘ëœ ì‚¬ë£Œ ìš”ì•½] : {text_data.strip()[:60]}...")

        # 2. ì „ì²˜ë¦¬
        processed_text = preprocess_text(text_data)

        # 3. ëª¨ë¸ ì˜ˆì¸¡
        # í™•ë¥ (Confidence) ê³„ì‚°
        prediction = model.predict([processed_text])[0]
        proba = model.predict_proba([processed_text]).max() * 100

        # 4. ê²°ê³¼ ì¶œë ¥
        print("-" * 40)
        print(f"   ì´ë¦„: {name}")
        print(f"   ì˜ˆì¸¡ëœ ì„±í–¥: â­ {prediction} â­")
        print(f"   í™•ì‹ ë„: {proba:.2f}%")
        print("-" * 40)

# ì‹¤í–‰
if __name__ == "__main__":
    predict_tendency()

pip install google-generativeai requests beautifulsoup4

import google.generativeai as genai
import requests
from bs4 import BeautifulSoup
import random
import json
import time

# =========================================================
# 1. ì„¤ì • (API í‚¤ ì…ë ¥)
# =========================================================
# êµ¬ê¸€ AI Studioì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤ë¥¼ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”.
API_KEY = "AIzaSyBEmAHr7r1i2mt8yu6qjp3P79ErIfQrIfw"

genai.configure(api_key=API_KEY)

# ì‚¬ìš©í•  ëª¨ë¸ ì„¤ì • (gemini-1.5-flashê°€ ì†ë„ê°€ ë¹ ë¥´ê³  ì €ë ´í•˜ì—¬ ë¶„ë¥˜ ì‘ì—…ì— ì í•©í•©ë‹ˆë‹¤)
model = genai.GenerativeModel('gemini-1.5-flash')

# =========================================================
# 2. ì›¹ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ (ìë£Œ ìˆ˜ì§‘)
# =========================================================
def scrap_history_data(name):
    """
    ì¸ë¬¼ ì´ë¦„ì„ ë°›ì•„ ì˜¨ë¼ì¸ ë°±ê³¼ì‚¬ì „/DBì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ê¸ì–´ì˜µë‹ˆë‹¤.
    (êµ­ì‚¬í¸ì°¬ìœ„ì›íšŒ DBëŠ” êµ¬ì¡°ê°€ ë³µì¡í•˜ì—¬, ìŠ¤í¬ë˜í•‘ì´ ìš©ì´í•œ í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ URLì„ ì˜ˆì‹œë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.)
    """
    print(f"ğŸ” '{name}'ì— ëŒ€í•œ ìë£Œë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...")

    # í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì‚¬ì „ ê²€ìƒ‰ URL
    url = f"https://encykorea.aks.ac.kr/Article/Search/{name}"

    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, 'html.parser')

        # ê²€ìƒ‰ ê²°ê³¼ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ)
        # ë‹¨ìˆœíˆ í˜ì´ì§€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ê¸ì–´ì„œ AIì—ê²Œ ê±¸ëŸ¬ë‚´ë¼ê³  í•˜ëŠ” ê²ƒì´ ê°€ì¥ ê°•ë ¥í•œ ë°©ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.
        text_content = soup.get_text()

        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ AI í† í° ì œí•œì„ ê³ ë ¤í•´ ì ë‹¹íˆ ìë¦…ë‹ˆë‹¤.
        return text_content[:5000]

    except Exception as e:
        print(f"âŒ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# =========================================================
# 3. Gemini APIë¥¼ í™œìš©í•œ ë¶„ë¥˜ í•¨ìˆ˜
# =========================================================
def classify_figure_with_ai(name, context_text):
    """
    ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ë¥¼ Geminiì—ê²Œ ì£¼ê³  ì„±í–¥ ë¶„ë¥˜ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ¤– AIê°€ '{name}'ì˜ ì„±í–¥ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")

    # í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
    prompt = f"""
    ë‹¹ì‹ ì€ í•œêµ­ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ [í…ìŠ¤íŠ¸]ëŠ” '{name}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.
    ì´ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì´ ì¸ë¬¼ì˜ ì£¼ëœ í™œë™ ì„±í–¥ì„ ë‹¤ìŒ 5ê°€ì§€ë§Œ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.

    [ë¶„ë¥˜ ê¸°ì¤€]
    1. ë¬´ì¥íˆ¬ìŸë¡  (ë¬´ë ¥ìœ¼ë¡œ ë…ë¦½ ìŸì·¨)
    2. ì‹¤ë ¥ì–‘ì„±ë¡  (êµìœ¡, ì‚°ì—… ìœ¡ì„±)
    3. ì™¸êµë¡  (êµ­ì œ ì‚¬íšŒì— í˜¸ì†Œ)
    4. ì¹œì¼íŒŒ (ë°˜ë¯¼ì¡± í–‰ìœ„)
    5. ê¸°íƒ€ (ë¶„ë¥˜í•˜ê¸° ì–´ë µê±°ë‚˜ í•´ë‹¹ ì‹œê¸° ì¸ë¬¼ì´ ì•„ë‹˜)

    [ì¶œë ¥ í˜•ì‹]
    ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    {{
        "name": "{name}",
        "classification": "ë¶„ë¥˜ ê²°ê³¼",
        "reason": "íŒë‹¨ ì´ìœ ë¥¼ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½"
    }}

    [í…ìŠ¤íŠ¸]
    {context_text}
    """

    try:
        response = model.generate_content(prompt)
        # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ íŒŒì‹± (ê°€ë” AIê°€ ì¡ë‹´ì„ ì„ì„ ìˆ˜ ìˆì–´ì„œ ì²˜ë¦¬)
        result_text = response.text.replace("```json", "").replace("```", "").strip()
        return json.loads(result_text)
    except Exception as e:
        print(f"âŒ AI ë¶„ì„ ì˜¤ë¥˜: {e}")
        return None

# =========================================================
# 4. ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
# =========================================================
def main():
    # ëœë¤ìœ¼ë¡œ ë½‘ì„ ì¸ë¬¼ ë¦¬ìŠ¤íŠ¸
    history_figures = ["ê¹€êµ¬", "ì•ˆì°½í˜¸", "ì´ìŠ¹ë§Œ", "ì´ì™„ìš©", "ê¹€ì›ë´‰", "ì‹ ì±„í˜¸", "ìœ¤ë´‰ê¸¸", "ì†¡ì§„ìš°"]

    print("=" * 50)
    print("   ğŸš€ AI í•œêµ­ì‚¬ ì¸ë¬¼ ì„±í–¥ ë¶„ë¥˜ê¸° (Gemini API)   ")
    print("=" * 50)

    while True:
        user_input = input("\nğŸ‘‰ Enterë¥¼ ëˆ„ë¥´ë©´ ëœë¤ ì¸ë¬¼ì„ ë¶„ì„í•©ë‹ˆë‹¤ (ì¢…ë£Œí•˜ë ¤ë©´ 'q' ì…ë ¥): ")
        if user_input.lower() == 'q':
            break

        # 1. ëœë¤ ì¸ë¬¼ ì„ íƒ
        target_person = random.choice(history_figures)
        print(f"\nğŸ¯ ì„ íƒëœ ì¸ë¬¼: [{target_person}]")

        # 2. ìë£Œ ìˆ˜ì§‘ (ìŠ¤í¬ë˜í•‘)
        scraped_text = scrap_history_data(target_person)

        if not scraped_text or len(scraped_text) < 100:
            print("âš  ìë£Œë¥¼ ì¶©ë¶„íˆ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")
            continue

        # 3. AI ë¶„ì„ (API í˜¸ì¶œ)
        result = classify_figure_with_ai(target_person, scraped_text)

        # 4. ê²°ê³¼ ì¶œë ¥
        if result:
            print("\n" + "-"*40)
            print(f" ğŸ·ï¸  ì´ë¦„: {result['name']}")
            print(f" ğŸš© ì„±í–¥: {result['classification']}")
            print(f" ğŸ’¡ ì´ìœ : {result['reason']}")
            print("-" * 40)

        time.sleep(1) # API í˜¸ì¶œ ì œí•œ ë°©ì§€

if __name__ == "__main__":
    main()

# [1] ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°•ì œ ì—…ë°ì´íŠ¸ (ì´ ì½”ë“œë¥¼ ê°€ì¥ ë¨¼ì € ì‹¤í–‰í•´ì•¼ 404 ì˜¤ë¥˜ê°€ ì‚¬ë¼ì§‘ë‹ˆë‹¤)
!pip install -U google-generativeai requests beautifulsoup4

import google.generativeai as genai
import requests
from bs4 import BeautifulSoup
import random
import json
import time
import os

# =========================================================
# 2. ì„¤ì • (API í‚¤ ì…ë ¥)
# =========================================================
# ë°œê¸‰ë°›ì€ API í‚¤ë¥¼ ë”°ì˜´í‘œ ì•ˆì— ë„£ì–´ì£¼ì„¸ìš”
API_KEY = "AIzaSyBEmAHr7r1i2mt8yu6qjp3P79ErIfQrIfw"

genai.configure(api_key=API_KEY)

# ëª¨ë¸ ì„¤ì • (í˜¹ì‹œ flash ëª¨ë¸ì´ ì•ˆ ë˜ë©´ pro ëª¨ë¸ë¡œ ìë™ ì „í™˜í•˜ë„ë¡ ì˜ˆì™¸ì²˜ë¦¬ ì¶”ê°€í•¨)
def get_model():
    try:
        model = genai.GenerativeModel('gemini-2.5-flash')
        return model
    except:
        print("âš ï¸ Flash ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨, ê¸°ë³¸ Pro ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        return genai.GenerativeModel('gemini-pro')

model = get_model()

# =========================================================
# 3. ì›¹ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜
# =========================================================
def scrap_history_data(name):
    print(f"ğŸ” '{name}'ì— ëŒ€í•œ ìë£Œë¥¼ í•œêµ­ë¯¼ì¡±ë¬¸í™”ëŒ€ë°±ê³¼ì—ì„œ ì°¾ëŠ” ì¤‘...")
    url = f"https://encykorea.aks.ac.kr/Article/Search/{name}"

    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)
        if response.status_code != 200: return None
        soup = BeautifulSoup(response.text, 'html.parser')
        text_content = soup.get_text()
        return text_content[:5000] # ë„ˆë¬´ ê¸¸ë©´ ìë¦„
    except Exception as e:
        print(f"âŒ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
        return None

# =========================================================
# 4. AI ë¶„ë¥˜ í•¨ìˆ˜
# =========================================================
def classify_figure_with_ai(name, context_text):
    print(f"ğŸ¤– AIê°€ '{name}'ì˜ ì„±í–¥ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...")

    prompt = f"""
    ë‹¹ì‹ ì€ í•œêµ­ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ [í…ìŠ¤íŠ¸]ëŠ” '{name}'ì— ëŒ€í•œ ìë£Œì…ë‹ˆë‹¤.
    ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì´ ì¸ë¬¼ì˜ ì„±í–¥ì„ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ê³  ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.

    [ë¶„ë¥˜ ê¸°ì¤€]
    1. ë¬´ì¥íˆ¬ìŸë¡  (ë¬´ë ¥ ë…ë¦½ ìš´ë™)
    2. ì‹¤ë ¥ì–‘ì„±ë¡  (êµìœ¡/ì‚°ì—… ìœ¡ì„±)
    3. ì™¸êµë¡  (êµ­ì œ ì‚¬íšŒ í˜¸ì†Œ)
    4. ì¹œì¼íŒŒ (ë°˜ë¯¼ì¡± í–‰ìœ„)
    5. ê¸°íƒ€ (í•´ë‹¹ ì—†ìŒ)

    [ì¶œë ¥ í˜•ì‹ - JSON]
    {{
        "name": "{name}",
        "classification": "ë¶„ë¥˜ ê²°ê³¼",
        "reason": "íŒë‹¨ ì´ìœ  í•œ ì¤„ ìš”ì•½"
    }}

    [í…ìŠ¤íŠ¸]
    {context_text}
    """

    try:
        response = model.generate_content(prompt)
        # JSON í˜•ì‹ ì •ì œ
        text = response.text.replace("```json", "").replace("```", "").strip()
        return json.loads(text)
    except Exception as e:
        print(f"âŒ AI ì‘ë‹µ ì˜¤ë¥˜: {e}")
        return None

# =========================================================
# 5. ë©”ì¸ ì‹¤í–‰ (ì…ë ¥ ë¡œì§ ìˆ˜ì •ë¨)
# =========================================================
def main():
    history_figures = ["ê¹€êµ¬", "ì•ˆì°½í˜¸", "ì´ìŠ¹ë§Œ", "ì´ì™„ìš©", "ê¹€ì›ë´‰", "ì‹ ì±„í˜¸", "ìœ¤ë´‰ê¸¸"]

    print("\n" + "="*50)
    print("   ğŸš€ AI í•œêµ­ì‚¬ ì¸ë¬¼ ì„±í–¥ ë¶„ë¥˜ê¸° (ìˆ˜ì •ë²„ì „)")
    print("="*50)

    while True:
        # ì„ ìƒë‹˜ì˜ ì˜ë„ëŒ€ë¡œ ì…ë ¥ì„ ë°›ë„ë¡ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
        user_input = input("\nğŸ‘‰ ì¸ë¬¼ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” (ê·¸ëƒ¥ ì—”í„°ì¹˜ë©´ ëœë¤, 'q'ëŠ” ì¢…ë£Œ): ").strip()

        if user_input.lower() == 'q':
            print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ì…ë ¥ê°’ì´ ìˆìœ¼ë©´ ê·¸ ì‚¬ëŒì„, ì—†ìœ¼ë©´ ëœë¤ìœ¼ë¡œ ì„ íƒ
        if user_input:
            target_person = user_input
        else:
            target_person = random.choice(history_figures)

        print(f"\nğŸ¯ ë¶„ì„ ëŒ€ìƒ: [{target_person}]")

        # ìë£Œ ìˆ˜ì§‘
        scraped_text = scrap_history_data(target_person)

        if not scraped_text or len(scraped_text) < 50:
            print("âš  ìë£Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ì¸ë¬¼ëª…ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            continue

        # AI ë¶„ì„
        result = classify_figure_with_ai(target_person, scraped_text)

        if result:
            print("-" * 40)
            print(f" ğŸ·ï¸  ì´ë¦„: {result.get('name')}")
            print(f" ğŸš© ì„±í–¥: {result.get('classification')}")
            print(f" ğŸ’¡ ì´ìœ : {result.get('reason')}")
            print("-" * 40)

        time.sleep(1)

if __name__ == "__main__":
    main()

